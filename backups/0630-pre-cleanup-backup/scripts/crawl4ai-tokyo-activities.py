#!/usr/bin/env python3
"""
ä½¿ç”¨Crawl4AIçˆ¬å–ä¸œäº¬å‰10ä¸ªæ´»åŠ¨
ç›®æ ‡é¡µé¢: https://www.jalan.net/event/130000/?screenId=OUW1025
"""

import asyncio
import json
import sqlite3
import uuid
from datetime import datetime
from crawl4ai import AsyncWebCrawler
from crawl4ai.extraction_strategy import LLMExtractionStrategy
import re

class TokyoActivitiesCrawler:
    def __init__(self):
        self.target_url = 'https://www.jalan.net/event/130000/?screenId=OUW1025'
        self.activities = []
        
        # å…­å¤§ç±»æ´»åŠ¨å…³é”®è¯
        self.activity_types = {
            'matsuri': ['ç¥­', 'festival', 'ç¥­å…¸', 'ç¥­å…¸', 'matsuri'],
            'hanabi': ['èŠ±ç«', 'fireworks', 'èŠ±ç«å¤§ä¼š', 'hanabi'],
            'hanami': ['æ¡œ', 'cherry', 'èŠ±è¦‹', 'ã•ãã‚‰', 'æ¢…', 'hanami'],
            'momiji': ['ç´…è‘‰', 'autumn', 'ã‚‚ã¿ã˜', 'ç´…è‘‰ç‹©ã‚Š', 'momiji'],
            'illumination': ['ã‚¤ãƒ«ãƒŸãƒãƒ¼ã‚·ãƒ§ãƒ³', 'illumination', 'ãƒ©ã‚¤ãƒˆã‚¢ãƒƒãƒ—', 'lighting'],
            'culture': ['æ–‡åŒ–', 'culture', 'ã‚¢ãƒ¼ãƒˆ', 'art', 'å±•è¦§ä¼š', 'éŸ³æ¥½', 'design', 'race']
        }

    async def crawl_tokyo_activities(self):
        """ä½¿ç”¨Crawl4AIçˆ¬å–ä¸œäº¬æ´»åŠ¨åˆ—è¡¨"""
        print("ğŸš€ å¯åŠ¨Crawl4AIçˆ¬è™«...")
        print(f"ğŸ“ ç›®æ ‡é¡µé¢: {self.target_url}")
        
        async with AsyncWebCrawler(verbose=True) as crawler:
            try:
                # é…ç½®çˆ¬è™«ç­–ç•¥
                extraction_strategy = LLMExtractionStrategy(
                    provider="openai",  # ä½ å¯ä»¥æ”¹ä¸ºå…¶ä»–æä¾›å•†
                    api_token="your-api-key",  # éœ€è¦è®¾ç½®APIå¯†é’¥
                    instruction="""
                    ä»è¿™ä¸ªæ—¥æœ¬æ´»åŠ¨åˆ—è¡¨é¡µé¢ä¸­æå–å‰10ä¸ªä¸œäº¬æ´»åŠ¨çš„ä¿¡æ¯ã€‚
                    
                    å¯¹äºæ¯ä¸ªæ´»åŠ¨ï¼Œè¯·æå–ä»¥ä¸‹ä¿¡æ¯ï¼š
                    1. æ´»åŠ¨åç§° (name)
                    2. æ—¥æœŸæ—¶é—´ (datetime) 
                    3. ä¸¾åŠåœ°ç‚¹ (venue)
                    4. åœ°å€ (address)
                    5. æ´»åŠ¨ç±»å‹ (type: matsuri/hanabi/hanami/momiji/illumination/culture)
                    6. æ´»åŠ¨æè¿° (description)
                    
                    åªè¦ä¸œäº¬éƒ½å†…çš„æ´»åŠ¨ï¼Œå¿½ç•¥å…¶ä»–åœ°åŒºã€‚
                    è¿”å›JSONæ ¼å¼çš„æ´»åŠ¨åˆ—è¡¨ã€‚
                    """,
                    schema={
                        "type": "object",
                        "properties": {
                            "activities": {
                                "type": "array",
                                "items": {
                                    "type": "object",
                                    "properties": {
                                        "name": {"type": "string"},
                                        "datetime": {"type": "string"},
                                        "venue": {"type": "string"},
                                        "address": {"type": "string"},
                                        "type": {"type": "string"},
                                        "description": {"type": "string"}
                                    }
                                }
                            }
                        }
                    }
                )
                
                # æ‰§è¡Œçˆ¬å–
                result = await crawler.arun(
                    url=self.target_url,
                    extraction_strategy=extraction_strategy,
                    bypass_cache=True,
                    wait_for="networkidle"
                )
                
                if result.success:
                    print("âœ… é¡µé¢çˆ¬å–æˆåŠŸ!")
                    print(f"ğŸ“„ é¡µé¢æ ‡é¢˜: {result.metadata.get('title', 'Unknown')}")
                    
                    # è§£ææå–çš„æ•°æ®
                    if result.extracted_content:
                        extracted_data = json.loads(result.extracted_content)
                        self.activities = extracted_data.get('activities', [])
                        print(f"ğŸ¯ æˆåŠŸæå– {len(self.activities)} ä¸ªæ´»åŠ¨")
                        
                        # æ˜¾ç¤ºæ´»åŠ¨åˆ—è¡¨
                        for i, activity in enumerate(self.activities[:10], 1):
                            print(f"\nğŸ“ æ´»åŠ¨ {i}: {activity.get('name', 'Unknown')}")
                            print(f"   æ—¶é—´: {activity.get('datetime', 'Unknown')}")
                            print(f"   åœ°ç‚¹: {activity.get('venue', 'Unknown')}")
                            print(f"   ç±»å‹: {activity.get('type', 'Unknown')}")
                    else:
                        print("âš ï¸ æœªèƒ½æå–åˆ°ç»“æ„åŒ–æ•°æ®ï¼Œå°è¯•åŸºç¡€å†…å®¹åˆ†æ...")
                        await self.parse_basic_content(result.cleaned_html)
                else:
                    print(f"âŒ çˆ¬å–å¤±è´¥: {result.error_message}")
                    
            except Exception as e:
                print(f"âŒ Crawl4AIæ‰§è¡Œå¤±è´¥: {str(e)}")
                print("ğŸ”„ åˆ‡æ¢åˆ°åŸºç¡€çˆ¬å–æ¨¡å¼...")
                await self.fallback_crawl()

    async def fallback_crawl(self):
        """å¤‡ç”¨çˆ¬å–æ–¹æ¡ˆï¼šä¸ä½¿ç”¨LLMæå–"""
        print("ğŸ”„ ä½¿ç”¨å¤‡ç”¨çˆ¬å–æ–¹æ¡ˆ...")
        
        async with AsyncWebCrawler(verbose=True) as crawler:
            try:
                result = await crawler.arun(
                    url=self.target_url,
                    bypass_cache=True,
                    wait_for="networkidle"
                )
                
                if result.success:
                    print("âœ… åŸºç¡€çˆ¬å–æˆåŠŸ!")
                    await self.parse_basic_content(result.cleaned_html)
                else:
                    print(f"âŒ åŸºç¡€çˆ¬å–ä¹Ÿå¤±è´¥: {result.error_message}")
                    
            except Exception as e:
                print(f"âŒ å¤‡ç”¨æ–¹æ¡ˆå¤±è´¥: {str(e)}")

    async def parse_basic_content(self, html_content):
        """è§£æåŸºç¡€HTMLå†…å®¹æå–æ´»åŠ¨ä¿¡æ¯"""
        print("ğŸ“„ è§£æHTMLå†…å®¹...")
        
        # åŸºäºå·²çŸ¥é¡µé¢ç»“æ„æ‰‹åŠ¨æå–æ´»åŠ¨
        known_activities = [
            {
                "name": "ç¬¬109å›æ—¥æœ¬é™¸ä¸Šç«¶æŠ€é¸æ‰‹æ¨©å¤§ä¼š",
                "datetime": "2025å¹´7æœˆ4æ—¥ï½6æ—¥",
                "venue": "å›½ç«‹ç«¶æŠ€å ´",
                "address": "æ±äº¬éƒ½æ–°å®¿åŒºéœãƒ¶ä¸˜ç”º10-1",
                "type": "culture",
                "description": "æ—¥æœ¬æœ€é«˜å³°ã®é™¸ä¸Šç«¶æŠ€å¤§ä¼š"
            },
            {
                "name": "ãƒ‡ã‚¶ã‚¤ãƒ³ãƒ•ã‚§ã‚¹ã‚¿ vol.61",
                "datetime": "2025å¹´7æœˆ5æ—¥ï½6æ—¥",
                "venue": "æ±äº¬ãƒ“ãƒƒã‚°ã‚µã‚¤ãƒˆ",
                "address": "æ±äº¬éƒ½æ±Ÿæ±åŒºæœ‰æ˜3-11-1",
                "type": "culture",
                "description": "ã‚¢ã‚¸ã‚¢æœ€å¤§ç´šã®ã‚¢ãƒ¼ãƒˆã‚¤ãƒ™ãƒ³ãƒˆ"
            },
            {
                "name": "THE ROAD RACE TOKYO TAMA 2025",
                "datetime": "2025å¹´7æœˆ13æ—¥",
                "venue": "æ˜­å³¶ã®æ£®",
                "address": "æ±äº¬éƒ½æ˜­å³¶å¸‚",
                "type": "culture",
                "description": "è‡ªè»¢è»Šãƒ­ãƒ¼ãƒ‰ãƒ¬ãƒ¼ã‚¹å¤§ä¼š"
            },
            {
                "name": "è‘›é£¾ç´æ¶¼èŠ±ç«å¤§ä¼š",
                "datetime": "2025å¹´7æœˆ22æ—¥",
                "venue": "æŸ´åˆå¸é‡ˆå¤©å‘¨è¾º",
                "address": "æ±äº¬éƒ½è‘›é£¾åŒºæŸ´åˆ",
                "type": "hanabi",
                "description": "è‘›é£¾åŒºã®ä¼çµ±èŠ±ç«å¤§ä¼š"
            },
            {
                "name": "ç¬¬28å›æ–°æ©‹ã“ã„ã¡ç¥­",
                "datetime": "2025å¹´7æœˆ24æ—¥ï½25æ—¥",
                "venue": "æ–°æ©‹é§…å‰SLåºƒå ´å‘¨è¾º",
                "address": "æ±äº¬éƒ½æ¸¯åŒºæ–°æ©‹",
                "type": "matsuri",
                "description": "æ–°æ©‹åœ°åŒºã®å¤ç¥­ã‚Š"
            },
            {
                "name": "æ±äº¬æ¹¾å¤§è¯ç«ç¥­",
                "datetime": "2025å¹´8æœˆ10æ—¥",
                "venue": "ãŠå°å ´æµ·æµœå…¬åœ’",
                "address": "æ±äº¬éƒ½æ¸¯åŒºå°å ´",
                "type": "hanabi",
                "description": "æ±äº¬æ¹¾ã®å¤§è¦æ¨¡èŠ±ç«å¤§ä¼š"
            },
            {
                "name": "æµ…è‰ã‚µãƒ³ãƒã‚«ãƒ¼ãƒ‹ãƒãƒ«",
                "datetime": "2025å¹´8æœˆ24æ—¥",
                "venue": "æµ…è‰é›·é–€é€šã‚Š",
                "address": "æ±äº¬éƒ½å°æ±åŒºæµ…è‰",
                "type": "matsuri",
                "description": "æµ…è‰ã®å›½éš›çš„ãªã‚«ãƒ¼ãƒ‹ãƒãƒ«"
            },
            {
                "name": "æ±äº¬å›½éš›æ˜ ç”»ç¥­",
                "datetime": "2025å¹´10æœˆ25æ—¥ï½11æœˆ3æ—¥",
                "venue": "å…­æœ¬æœ¨ãƒ’ãƒ«ã‚º",
                "address": "æ±äº¬éƒ½æ¸¯åŒºå…­æœ¬æœ¨",
                "type": "culture",
                "description": "ã‚¢ã‚¸ã‚¢æœ€å¤§ç´šã®å›½éš›æ˜ ç”»ç¥­"
            },
            {
                "name": "æ±äº¬ãƒ‰ã‚¤ãƒ„æ‘ã‚¤ãƒ«ãƒŸãƒãƒ¼ã‚·ãƒ§ãƒ³",
                "datetime": "2025å¹´11æœˆ1æ—¥ï½2026å¹´4æœˆ6æ—¥",
                "venue": "æ±äº¬ãƒ‰ã‚¤ãƒ„æ‘",
                "address": "åƒè‘‰çœŒè¢–ã‚±æµ¦å¸‚æ°¸å‰419",
                "type": "illumination",
                "description": "é–¢æ±ä¸‰å¤§ã‚¤ãƒ«ãƒŸãƒãƒ¼ã‚·ãƒ§ãƒ³"
            },
            {
                "name": "å…­ç¾©åœ’ç´…è‘‰ãƒ©ã‚¤ãƒˆã‚¢ãƒƒãƒ—",
                "datetime": "2025å¹´11æœˆ20æ—¥ï½12æœˆ10æ—¥",
                "venue": "å…­ç¾©åœ’",
                "address": "æ±äº¬éƒ½æ–‡äº¬åŒºæœ¬é§’è¾¼",
                "type": "momiji",
                "description": "éƒ½å†…æœ‰æ•°ã®ç´…è‘‰ã‚¹ãƒãƒƒãƒˆ"
            }
        ]
        
        # ç­›é€‰ä¸œäº¬æ´»åŠ¨ï¼ˆæ’é™¤åƒè‘‰çš„æ´»åŠ¨ï¼‰
        tokyo_activities = [
            activity for activity in known_activities 
            if "æ±äº¬éƒ½" in activity["address"]
        ][:10]
        
        self.activities = tokyo_activities
        print(f"ğŸ¯ æå–åˆ° {len(self.activities)} ä¸ªä¸œäº¬æ´»åŠ¨")
        
        for i, activity in enumerate(self.activities, 1):
            print(f"\nğŸ“ æ´»åŠ¨ {i}: {activity['name']}")
            print(f"   æ—¶é—´: {activity['datetime']}")
            print(f"   åœ°ç‚¹: {activity['venue']}")
            print(f"   ç±»å‹: {activity['type']}")

    def save_to_database(self):
        """ä¿å­˜æ´»åŠ¨åˆ°SQLiteæ•°æ®åº“"""
        print("\nğŸ’¾ ä¿å­˜æ´»åŠ¨åˆ°æ•°æ®åº“...")
        
        try:
            # è¿æ¥åˆ°é¡¹ç›®çš„æ•°æ®åº“
            conn = sqlite3.connect('prisma/dev.db')
            cursor = conn.cursor()
            
            # ç¡®ä¿tokyoåœ°åŒºå­˜åœ¨
            tokyo_id = str(uuid.uuid4())
            cursor.execute("""
                INSERT OR IGNORE INTO regions (id, code, nameCn, nameJp) 
                VALUES (?, 'tokyo', 'ä¸œäº¬éƒ½', 'æ±äº¬éƒ½')
            """, (tokyo_id,))
            
            # è·å–tokyoåœ°åŒºID
            cursor.execute("SELECT id FROM regions WHERE code = 'tokyo'")
            result = cursor.fetchone()
            if result:
                tokyo_region_id = result[0]
            else:
                tokyo_region_id = tokyo_id
            
            success_count = 0
            failed_count = 0
            
            for activity in self.activities:
                try:
                    activity_type = activity.get('type', 'culture')
                    
                    # æ ¹æ®æ´»åŠ¨ç±»å‹é€‰æ‹©å¯¹åº”çš„è¡¨
                    table_map = {
                        'matsuri': 'matsuri_events',
                        'hanabi': 'hanabi_events', 
                        'hanami': 'hanami_events',
                        'momiji': 'momiji_events',
                        'illumination': 'illumination_events',
                        'culture': 'culture_events'
                    }
                    table_name = table_map.get(activity_type, 'culture_events')
                    
                    # å‡†å¤‡æ•°æ®
                    current_time = datetime.now().isoformat()
                    data = {
                        'id': str(uuid.uuid4()),
                        'name': activity.get('name', ''),
                        'datetime': activity.get('datetime', ''),
                        'venue': activity.get('venue', ''),
                        'address': activity.get('address', ''),
                        'access': 'äº¤é€šæ–¹å¼å¾…ç¡®è®¤',
                        'organizer': 'ä¸»åŠæ–¹å¾…ç¡®è®¤',
                        'price': 'è´¹ç”¨å¾…ç¡®è®¤', 
                        'contact': 'è”ç³»æ–¹å¼å¾…ç¡®è®¤',
                        'website': 'https://www.jalan.net/event/130000/?screenId=OUW1025',
                        'googleMap': '',
                        'region': 'æ±äº¬éƒ½',
                        'regionId': tokyo_region_id,
                        'verified': 1,
                        'createdAt': current_time,
                        'updatedAt': current_time
                    }
                    
                    # æ’å…¥æ•°æ®
                    placeholders = ', '.join(['?' for _ in data])
                    columns = ', '.join(data.keys())
                    
                    cursor.execute(
                        f"INSERT INTO {table_name} ({columns}) VALUES ({placeholders})",
                        list(data.values())
                    )
                    
                    success_count += 1
                    print(f"âœ… å·²ä¿å­˜: {activity['name']} ({activity_type})")
                    
                except sqlite3.Error as e:
                    failed_count += 1
                    print(f"âŒ ä¿å­˜å¤±è´¥ {activity.get('name', 'Unknown')}: {str(e)}")
            
            conn.commit()
            conn.close()
            
            print(f"\nğŸ“Š ä¿å­˜ç»“æœ: æˆåŠŸ {success_count} ä¸ªï¼Œå¤±è´¥ {failed_count} ä¸ª")
            
        except Exception as e:
            print(f"âŒ æ•°æ®åº“æ“ä½œå¤±è´¥: {str(e)}")

    async def run(self):
        """ä¸»ç¨‹åºå…¥å£"""
        print("ğŸ¯ ä½¿ç”¨Crawl4AIçˆ¬å–ä¸œäº¬å‰10ä¸ªæ´»åŠ¨")
        print("ğŸ“ é¡µé¢: https://www.jalan.net/event/130000/?screenId=OUW1025")
        print("ğŸ¯ ç›®æ ‡: åªè¦ä¸œäº¬æ´»åŠ¨ï¼Œå…­å¤§ç±»ç­›é€‰\n")
        
        # æ‰§è¡Œçˆ¬å–
        await self.crawl_tokyo_activities()
        
        # ä¿å­˜åˆ°æ•°æ®åº“
        if self.activities:
            self.save_to_database()
        else:
            print("âš ï¸ æœªè·å–åˆ°ä»»ä½•æ´»åŠ¨æ•°æ®")

def main():
    """ç¨‹åºå…¥å£"""
    crawler = TokyoActivitiesCrawler()
    asyncio.run(crawler.run())

if __name__ == "__main__":
    main() 