#!/usr/bin/env python3
"""
çœŸæ­£ä»Jalané¡µé¢æå–ä¸œäº¬æ´»åŠ¨æ•°æ®
ä¸¥æ ¼ä»æŒ‡å®šé¡µé¢è·å–çœŸå®ä¿¡æ¯ï¼Œç¦æ­¢ç¡¬ç¼–ç 
"""

import asyncio
import json
import sqlite3
import uuid
from datetime import datetime
from crawl4ai import AsyncWebCrawler
from bs4 import BeautifulSoup
import re

class RealJalanCrawler:
    def __init__(self):
        self.target_url = 'https://www.jalan.net/event/130000/?screenId=OUW1025'
        self.activities = []
        
    async def crawl_real_data(self):
        """çœŸæ­£ä»é¡µé¢æå–æ•°æ®"""
        print("ğŸ” æ­£åœ¨ä»Jalané¡µé¢æå–çœŸå®æ•°æ®...")
        print(f"ğŸ“ ç›®æ ‡é¡µé¢: {self.target_url}")
        
        async with AsyncWebCrawler(verbose=True) as crawler:
            try:
                # çˆ¬å–é¡µé¢
                result = await crawler.arun(
                    url=self.target_url,
                    bypass_cache=True,
                    wait_for="networkidle"
                )
                
                if result.success:
                    print("âœ… é¡µé¢çˆ¬å–æˆåŠŸ!")
                    print(f"ğŸ“„ é¡µé¢æ ‡é¢˜: {result.metadata.get('title', 'Unknown')}")
                    
                    # è¾“å‡ºåŸå§‹HTMLç”¨äºè°ƒè¯•
                    print("\nğŸ“„ å¼€å§‹åˆ†æé¡µé¢å†…å®¹...")
                    
                    # ä½¿ç”¨BeautifulSoupè§£æHTML
                    soup = BeautifulSoup(result.cleaned_html, 'html.parser')
                    
                    # æŸ¥æ‰¾æ´»åŠ¨åˆ—è¡¨å®¹å™¨
                    self.extract_activities_from_soup(soup)
                    
                    if not self.activities:
                        print("âš ï¸ æœªæ‰¾åˆ°æ´»åŠ¨æ•°æ®ï¼Œè¾“å‡ºé¡µé¢ç»“æ„ç”¨äºè°ƒè¯•...")
                        self.debug_page_structure(soup)
                        
                else:
                    print(f"âŒ é¡µé¢çˆ¬å–å¤±è´¥: {result.error_message}")
                    
            except Exception as e:
                print(f"âŒ çˆ¬å–è¿‡ç¨‹å¤±è´¥: {str(e)}")

    def extract_activities_from_soup(self, soup):
        """ä»BeautifulSoupå¯¹è±¡ä¸­æå–æ´»åŠ¨ä¿¡æ¯"""
        print("ğŸ” åœ¨é¡µé¢ä¸­æŸ¥æ‰¾æ´»åŠ¨åˆ—è¡¨...")
        
        # å°è¯•å¤šç§å¯èƒ½çš„æ´»åŠ¨å®¹å™¨é€‰æ‹©å™¨
        selectors = [
            'div[class*="item"]',
            'li[class*="item"]',
            'div[class*="event"]',
            'li[class*="event"]',
            'article',
            '.ranking-item',
            '.event-item',
            '.list-item'
        ]
        
        activities_found = False
        
        for selector in selectors:
            elements = soup.select(selector)
            if elements:
                print(f"âœ… æ‰¾åˆ° {len(elements)} ä¸ªå¯èƒ½çš„æ´»åŠ¨å…ƒç´  (é€‰æ‹©å™¨: {selector})")
                
                for i, element in enumerate(elements[:20]):  # é™åˆ¶å‰20ä¸ª
                    activity_data = self.extract_single_activity(element, i + 1)
                    if activity_data and self.is_tokyo_activity(activity_data):
                        self.activities.append(activity_data)
                        activities_found = True
                        print(f"ğŸ“ æå–æ´»åŠ¨ {len(self.activities)}: {activity_data.get('name', 'Unknown')}")
                        
                        if len(self.activities) >= 10:
                            break
                
                if activities_found:
                    break
        
        if not activities_found:
            print("âŒ æœªæ‰¾åˆ°ä»»ä½•æ´»åŠ¨ä¿¡æ¯")

    def extract_single_activity(self, element, index):
        """ä»å•ä¸ªå…ƒç´ ä¸­æå–æ´»åŠ¨ä¿¡æ¯"""
        try:
            # è·å–æ‰€æœ‰æ–‡æœ¬å†…å®¹
            text_content = element.get_text(strip=True)
            
            # æŸ¥æ‰¾æ´»åŠ¨åç§°
            name_element = element.find(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])
            if name_element:
                name = name_element.get_text(strip=True)
            else:
                # å°è¯•ä»é“¾æ¥æˆ–ç¬¬ä¸€è¡Œæ–‡æœ¬è·å–åç§°
                link = element.find('a')
                if link and link.get_text(strip=True):
                    name = link.get_text(strip=True)
                else:
                    lines = text_content.split('\n')
                    name = lines[0] if lines else f"æ´»åŠ¨{index}"
            
            # æŸ¥æ‰¾æ—¥æœŸä¿¡æ¯
            date_patterns = [
                r'\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥',
                r'\d{1,2}æœˆ\d{1,2}æ—¥',
                r'\d{4}/\d{1,2}/\d{1,2}',
                r'\d{1,2}/\d{1,2}'
            ]
            
            datetime_info = ""
            for pattern in date_patterns:
                match = re.search(pattern, text_content)
                if match:
                    datetime_info = match.group()
                    break
            
            # æŸ¥æ‰¾åœ°ç‚¹ä¿¡æ¯
            venue_keywords = ['ä¼šå ´', 'å ´æ‰€', 'é–‹å‚¬åœ°', 'é§…', 'å…¬åœ’', 'é¤¨', 'å ´', 'åºƒå ´']
            venue = ""
            for keyword in venue_keywords:
                if keyword in text_content:
                    # å°è¯•æå–åŒ…å«å…³é”®è¯çš„å¥å­
                    sentences = text_content.split('ã€‚')
                    for sentence in sentences:
                        if keyword in sentence:
                            venue = sentence.strip()
                            break
                    if venue:
                        break
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«ä¸œäº¬ç›¸å…³ä¿¡æ¯
            tokyo_indicators = text_content.lower()
            is_tokyo = any(keyword in tokyo_indicators for keyword in ['æ±äº¬', 'tokyo', 'æ–°å®¿', 'æ¸‹è°·', 'æµ…è‰', 'ä¸Šé‡', 'æ± è¢‹'])
            
            if name and len(name) > 2:  # ç¡®ä¿åç§°æœ‰æ„ä¹‰
                return {
                    'name': name[:100],  # é™åˆ¶é•¿åº¦
                    'datetime': datetime_info,
                    'venue': venue[:100] if venue else 'ä¼šå ´æœªç¢ºèª',
                    'address': 'æ±äº¬éƒ½' if is_tokyo else 'åœ°å€æœªç¢ºèª',
                    'raw_text': text_content[:200],  # ä¿ç•™åŸå§‹æ–‡æœ¬ç”¨äºè°ƒè¯•
                    'is_tokyo': is_tokyo
                }
            
            return None
            
        except Exception as e:
            print(f"âš ï¸ æå–æ´»åŠ¨ {index} ä¿¡æ¯æ—¶å‡ºé”™: {str(e)}")
            return None

    def is_tokyo_activity(self, activity):
        """éªŒè¯æ˜¯å¦ä¸ºä¸œäº¬æ´»åŠ¨"""
        text = f"{activity.get('name', '')} {activity.get('venue', '')} {activity.get('address', '')} {activity.get('raw_text', '')}"
        tokyo_keywords = ['æ±äº¬', 'tokyo', 'æ±äº¬éƒ½', 'æ–°å®¿', 'æ¸‹è°·', 'éŠ€åº§', 'æµ…è‰', 'ä¸Šé‡', 'æ± è¢‹', 'ç§‹è‘‰åŸ']
        
        return any(keyword in text.lower() for keyword in tokyo_keywords) or activity.get('is_tokyo', False)

    def debug_page_structure(self, soup):
        """è°ƒè¯•é¡µé¢ç»“æ„"""
        print("\nğŸ” é¡µé¢ç»“æ„è°ƒè¯•ä¿¡æ¯:")
        
        # æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„å®¹å™¨
        containers = soup.find_all(['div', 'section', 'article', 'ul', 'ol'])
        
        structure_info = {}
        for container in containers:
            class_name = ' '.join(container.get('class', []))
            if class_name:
                if class_name not in structure_info:
                    structure_info[class_name] = 0
                structure_info[class_name] += 1
        
        print("ğŸ“Š é¡µé¢ä¸­çš„ä¸»è¦CSSç±»:")
        for class_name, count in sorted(structure_info.items(), key=lambda x: x[1], reverse=True)[:20]:
            print(f"   .{class_name}: {count} ä¸ªå…ƒç´ ")
        
        # æŸ¥æ‰¾æ‰€æœ‰é“¾æ¥
        links = soup.find_all('a', href=True)
        event_links = [link for link in links if '/event/' in link.get('href', '')]
        
        print(f"\nğŸ”— æ‰¾åˆ° {len(event_links)} ä¸ªæ´»åŠ¨ç›¸å…³é“¾æ¥:")
        for i, link in enumerate(event_links[:10]):
            href = link.get('href', '')
            text = link.get_text(strip=True)[:50]
            print(f"   {i+1}. {text} â†’ {href}")

    async def run(self):
        """ä¸»ç¨‹åº"""
        print("ğŸ¯ çœŸå®Jalanæ•°æ®æå–å™¨")
        print("âš ï¸ ä¸¥æ ¼æ¨¡å¼ï¼šåªæå–é¡µé¢çœŸå®æ•°æ®ï¼Œç¦æ­¢ç¡¬ç¼–ç ")
        print(f"ğŸ“ ç›®æ ‡é¡µé¢: {self.target_url}\n")
        
        await self.crawl_real_data()
        
        print(f"\nğŸ“Š æå–ç»“æœ:")
        print(f"   æ€»æ´»åŠ¨æ•°: {len(self.activities)}")
        print(f"   ä¸œäº¬æ´»åŠ¨: {len([a for a in self.activities if self.is_tokyo_activity(a)])}")
        
        if self.activities:
            print("\nğŸ“‹ æå–åˆ°çš„æ´»åŠ¨åˆ—è¡¨:")
            for i, activity in enumerate(self.activities, 1):
                print(f"   {i}. {activity.get('name', 'Unknown')}")
                print(f"      æ—¶é—´: {activity.get('datetime', 'Unknown')}")
                print(f"      åœ°ç‚¹: {activity.get('venue', 'Unknown')}")
                print(f"      åŸæ–‡: {activity.get('raw_text', '')[:100]}...")
                print()
        else:
            print("âŒ æœªæå–åˆ°ä»»ä½•æ´»åŠ¨æ•°æ®")
            print("ğŸ’¡ å»ºè®®æ£€æŸ¥é¡µé¢ç»“æ„æˆ–é€‰æ‹©å™¨")

def main():
    crawler = RealJalanCrawler()
    asyncio.run(crawler.run())

if __name__ == "__main__":
    main() 