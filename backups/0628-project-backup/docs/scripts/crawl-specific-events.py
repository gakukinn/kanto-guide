#!/usr/bin/env python3
"""
ä¸œäº¬æ´»åŠ¨è¯¦ç»†ä¿¡æ¯çˆ¬å–å·¥å…·
ä¸“é—¨è·å–å‰10ä¸ªæ´»åŠ¨çš„10é¡¹å…·ä½“ä¿¡æ¯
"""

import asyncio
import json
import re
from datetime import datetime
from crawl4ai import AsyncWebCrawler
from crawl4ai.extraction_strategy import JsonCssExtractionStrategy

class SpecificEventsCrawler:
    def __init__(self):
        self.base_url = "https://www.jalan.net/event/130000/?screenId=OUW1025"
        self.target_fields = [
            "åç§°", "æ‰€åœ¨åœ°", "é–‹å‚¬æœŸé–“", "é–‹å‚¬å ´æ‰€", 
            "äº¤é€šã‚¢ã‚¯ã‚»ã‚¹", "ä¸»å‚¬", "æ–™é‡‘", "å•åˆã›å…ˆ", 
            "ãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸", "è°·æ­Œç½‘ç«™"
        ]
    
    async def extract_event_detail_links(self):
        """ä»ä¸»é¡µé¢æå–å‰10ä¸ªå…·ä½“æ´»åŠ¨çš„è¯¦æƒ…é“¾æ¥"""
        print("ğŸ” æ­£åœ¨æå–å…·ä½“æ´»åŠ¨çš„è¯¦æƒ…é“¾æ¥...")
        
        async with AsyncWebCrawler(verbose=True) as crawler:
            try:
                result = await crawler.arun(
                    url=self.base_url,
                    wait_for=3,
                    remove_overlay_elements=True
                )
                
                if result.success:
                    print("âœ… æˆåŠŸè·å–æ´»åŠ¨åˆ—è¡¨é¡µé¢")
                    
                    # ä»markdownä¸­è§£æå…·ä½“æ´»åŠ¨çš„è¯¦æƒ…é“¾æ¥
                    event_links = self.parse_specific_event_links(result.markdown)
                    return event_links[:10]  # è¿”å›å‰10ä¸ª
                else:
                    print(f"âŒ è·å–é¡µé¢å¤±è´¥: {result.error_message}")
                    return []
                    
            except Exception as e:
                print(f"âŒ å‘ç”Ÿé”™è¯¯: {e}")
                return []
    
    def parse_specific_event_links(self, markdown_content):
        """ä»markdownä¸­è§£æå…·ä½“æ´»åŠ¨çš„è¯¦æƒ…é“¾æ¥"""
        print("ğŸ” è§£æå…·ä½“æ´»åŠ¨é“¾æ¥...")
        
        event_links = []
        lines = markdown_content.split('\n')
        
        for line in lines:
            line = line.strip()
            
            # æŸ¥æ‰¾å½¢å¦‚ï¼š[æ´»åŠ¨åç§°](https://www.jalan.net/event/evt_æ•°å­—/) çš„é“¾æ¥
            link_match = re.search(r'\[([^\]]+)\]\((https://www\.jalan\.net/event/evt_\d+/)\)', line)
            if link_match:
                title = link_match.group(1)
                url = link_match.group(2)
                
                event_links.append({
                    'name': title,
                    'url': url,
                    'line_content': line
                })
                print(f"ğŸ“Œ æ‰¾åˆ°æ´»åŠ¨: {title}")
                
                if len(event_links) >= 10:
                    break
        
        print(f"ğŸ“Š æ€»å…±æ‰¾åˆ° {len(event_links)} ä¸ªå…·ä½“æ´»åŠ¨é“¾æ¥")
        return event_links
    
    async def crawl_event_specific_info(self, event_link):
        """çˆ¬å–å•ä¸ªæ´»åŠ¨çš„åé¡¹å…·ä½“ä¿¡æ¯"""
        print(f"ğŸ•·ï¸ æ­£åœ¨çˆ¬å–æ´»åŠ¨è¯¦æƒ…: {event_link['name']}")
        
        # å®šä¹‰ä¸“é—¨çš„æå–ç­–ç•¥ï¼Œé’ˆå¯¹æ´»åŠ¨è¯¦æƒ…é¡µé¢
        schema = {
            "name": "event_specific_info",
            "baseSelector": "body",
            "fields": [
                {"name": "event_title", "selector": "h1, .event-title, .page-title", "type": "text"},
                {"name": "period", "selector": ".period, .date, .schedule, .event-date", "type": "text"},
                {"name": "location_area", "selector": ".area, .prefecture, .region", "type": "text"},
                {"name": "venue", "selector": ".venue, .place, .location, .address", "type": "text"},
                {"name": "access", "selector": ".access, .transportation, .traffic", "type": "text"},
                {"name": "organizer", "selector": ".organizer, .sponsor, .host", "type": "text"},
                {"name": "fee", "selector": ".fee, .price, .cost, .charge", "type": "text"},
                {"name": "contact", "selector": ".contact, .inquiry, .tel, .phone", "type": "text"},
                {"name": "website", "selector": ".website, .homepage, .official", "type": "text"},
                {"name": "all_links", "selector": "a[href*='http']", "type": "attribute", "attribute": "href"}
            ]
        }
        
        extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)
        
        async with AsyncWebCrawler(verbose=True) as crawler:
            try:
                result = await crawler.arun(
                    url=event_link['url'],
                    extraction_strategy=extraction_strategy,
                    wait_for=4,
                    remove_overlay_elements=True
                )
                
                if result.success:
                    # è§£æå¹¶æ•´ç†åé¡¹ä¿¡æ¯
                    event_info = self.extract_ten_fields(result.markdown, result.extracted_content, event_link)
                    return event_info
                else:
                    print(f"âŒ çˆ¬å–å¤±è´¥: {result.error_message}")
                    return None
                    
            except Exception as e:
                print(f"âŒ çˆ¬å–æ—¶å‘ç”Ÿé”™è¯¯: {e}")
                return None
    
    def extract_ten_fields(self, markdown_content, extracted_json, event_link):
        """ä»çˆ¬å–å†…å®¹ä¸­æå–åé¡¹å…·ä½“ä¿¡æ¯"""
        print(f"ğŸ“Š æ­£åœ¨æå– {event_link['name']} çš„åé¡¹ä¿¡æ¯...")
        
        event_info = {
            "åç§°": "",
            "æ‰€åœ¨åœ°": "",
            "é–‹å‚¬æœŸé–“": "", 
            "é–‹å‚¬å ´æ‰€": "",
            "äº¤é€šã‚¢ã‚¯ã‚»ã‚¹": "",
            "ä¸»å‚¬": "",
            "æ–™é‡‘": "",
            "å•åˆã›å…ˆ": "",
            "ãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸": "",
            "è°·æ­Œç½‘ç«™": ""
        }
        
        # 1. åç§° - ä»é“¾æ¥ä¿¡æ¯æˆ–é¡µé¢æ ‡é¢˜è·å–
        event_info["åç§°"] = event_link['name']
        
        lines = markdown_content.split('\n')
        
        # é€è¡Œåˆ†æmarkdownå†…å®¹
        for i, line in enumerate(lines):
            line = line.strip()
            
            # 2. é–‹å‚¬æœŸé–“ - æŸ¥æ‰¾æ—¥æœŸä¿¡æ¯
            if any(keyword in line for keyword in ['æœŸé–“', 'æ—¥æ™‚', 'é–‹å‚¬æ—¥', 'å®Ÿæ–½æœŸé–“']):
                date_match = re.search(r'(\d{4}å¹´.*?[æ—¥æœˆ]|\d{4}/\d{1,2}/\d{1,2}.*?|\d{1,2}æœˆ\d{1,2}æ—¥.*?)', line)
                if date_match and not event_info["é–‹å‚¬æœŸé–“"]:
                    event_info["é–‹å‚¬æœŸé–“"] = date_match.group(1)
            
            # 3. é–‹å‚¬å ´æ‰€ - æŸ¥æ‰¾åœºæ‰€ä¿¡æ¯
            if any(keyword in line for keyword in ['å ´æ‰€', 'ä¼šå ´', 'é–‹å‚¬å ´æ‰€', 'å ´æ‰€ï¼š']):
                venue_match = re.search(r'(?:å ´æ‰€|ä¼šå ´|é–‹å‚¬å ´æ‰€)[:ï¼š]\s*(.+)', line)
                if venue_match and not event_info["é–‹å‚¬å ´æ‰€"]:
                    event_info["é–‹å‚¬å ´æ‰€"] = venue_match.group(1)
            
            # 4. æ‰€åœ¨åœ° - æŸ¥æ‰¾åœ°åŒºä¿¡æ¯
            if any(keyword in line for keyword in ['æ±äº¬éƒ½', 'æ‰€åœ¨åœ°', 'åœ°åŸŸ']):
                if 'æ±äº¬éƒ½' in line and not event_info["æ‰€åœ¨åœ°"]:
                    event_info["æ‰€åœ¨åœ°"] = "æ±äº¬éƒ½"
            
            # 5. äº¤é€šã‚¢ã‚¯ã‚»ã‚¹ - æŸ¥æ‰¾äº¤é€šä¿¡æ¯
            if any(keyword in line for keyword in ['ã‚¢ã‚¯ã‚»ã‚¹', 'äº¤é€š', 'ã‚¢ã‚¯ã‚»ã‚¹æ–¹æ³•', 'æœ€å¯„ã‚Šé§…']):
                access_match = re.search(r'(?:ã‚¢ã‚¯ã‚»ã‚¹|äº¤é€š)[:ï¼š]\s*(.+)', line)
                if access_match and not event_info["äº¤é€šã‚¢ã‚¯ã‚»ã‚¹"]:
                    event_info["äº¤é€šã‚¢ã‚¯ã‚»ã‚¹"] = access_match.group(1)
            
            # 6. ä¸»å‚¬ - æŸ¥æ‰¾ä¸»åŠæ–¹ä¿¡æ¯
            if any(keyword in line for keyword in ['ä¸»å‚¬', 'ä¸»å‚¬è€…', 'ä¸»å‚¬ï¼š']):
                organizer_match = re.search(r'ä¸»å‚¬[:ï¼š]\s*(.+)', line)
                if organizer_match and not event_info["ä¸»å‚¬"]:
                    event_info["ä¸»å‚¬"] = organizer_match.group(1)
            
            # 7. æ–™é‡‘ - æŸ¥æ‰¾è´¹ç”¨ä¿¡æ¯
            if any(keyword in line for keyword in ['æ–™é‡‘', 'å‚åŠ è²»', 'å…¥å ´æ–™', 'è²»ç”¨', 'ç„¡æ–™', 'å††']):
                if not event_info["æ–™é‡‘"] and ('å††' in line or 'ç„¡æ–™' in line):
                    event_info["æ–™é‡‘"] = line
            
            # 8. å•åˆã›å…ˆ - æŸ¥æ‰¾è”ç³»æ–¹å¼
            if any(keyword in line for keyword in ['å•åˆã›', 'é€£çµ¡å…ˆ', 'TEL', 'é›»è©±', 'ãŠå•ã„åˆã‚ã›']):
                contact_match = re.search(r'(?:å•åˆã›|é€£çµ¡å…ˆ|TEL|é›»è©±)[:ï¼š]\s*(.+)', line)
                if contact_match and not event_info["å•åˆã›å…ˆ"]:
                    event_info["å•åˆã›å…ˆ"] = contact_match.group(1)
            
            # 9. ãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸ - æŸ¥æ‰¾å®˜æ–¹ç½‘ç«™
            if 'http' in line and any(keyword in line for keyword in ['ãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸', 'å…¬å¼', 'HP', 'ã‚µã‚¤ãƒˆ']):
                url_match = re.search(r'(https?://[^\s)]+)', line)
                if url_match and not event_info["ãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸"]:
                    event_info["ãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸"] = url_match.group(1)
        
        # 10. è°·æ­Œç½‘ç«™ - å¦‚æœæœ‰å®˜ç½‘ï¼Œç”Ÿæˆè°·æ­Œæœç´¢é“¾æ¥
        if event_info["ãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸"]:
            search_query = f"site:{event_info['ãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸']}"
            event_info["è°·æ­Œç½‘ç«™"] = f"https://www.google.com/search?q={search_query}"
        elif event_info["åç§°"]:
            search_query = f"{event_info['åç§°']} æ±äº¬"
            event_info["è°·æ­Œç½‘ç«™"] = f"https://www.google.com/search?q={search_query.replace(' ', '+')}"
        
        # å¦‚æœæŸäº›å­—æ®µä¸ºç©ºï¼Œè®¾ç½®é»˜è®¤å€¼
        if not event_info["æ‰€åœ¨åœ°"]:
            event_info["æ‰€åœ¨åœ°"] = "æ±äº¬éƒ½"
        
        return event_info
    
    async def crawl_top_10_specific_events(self):
        """çˆ¬å–å‰10ä¸ªæ´»åŠ¨çš„åé¡¹å…·ä½“ä¿¡æ¯"""
        print("ğŸš€ å¼€å§‹çˆ¬å–å‰10ä¸ªæ´»åŠ¨çš„è¯¦ç»†ä¿¡æ¯...")
        
        # ç¬¬ä¸€æ­¥ï¼šè·å–æ´»åŠ¨è¯¦æƒ…é“¾æ¥
        event_links = await self.extract_event_detail_links()
        
        if not event_links:
            print("âŒ æœªèƒ½è·å–åˆ°æ´»åŠ¨é“¾æ¥")
            return
        
        print(f"ğŸ“‹ å‡†å¤‡çˆ¬å– {len(event_links)} ä¸ªæ´»åŠ¨çš„åé¡¹ä¿¡æ¯")
        
        # ç¬¬äºŒæ­¥ï¼šçˆ¬å–æ¯ä¸ªæ´»åŠ¨çš„åé¡¹ä¿¡æ¯
        all_events_info = []
        
        for i, event_link in enumerate(event_links, 1):
            print(f"\nğŸ“„ å¤„ç†ç¬¬ {i}/{len(event_links)} ä¸ªæ´»åŠ¨...")
            
            event_info = await self.crawl_event_specific_info(event_link)
            if event_info:
                all_events_info.append(event_info)
                
                # æ˜¾ç¤ºæå–åˆ°çš„ä¿¡æ¯
                print("âœ… æå–åˆ°çš„ä¿¡æ¯:")
                for field, value in event_info.items():
                    if value:
                        print(f"  {field}: {value}")
                
                print(f"ğŸ’¾ å·²ä¿å­˜æ´»åŠ¨ {i}")
            
            # æ·»åŠ å»¶è¿Ÿ
            if i < len(event_links):
                print("â±ï¸ ç­‰å¾…3ç§’...")
                await asyncio.sleep(3)
        
        # ç¬¬ä¸‰æ­¥ï¼šä¿å­˜æœ€ç»ˆç»“æœ
        final_result = {
            'crawl_time': datetime.now().isoformat(),
            'source_url': self.base_url,
            'total_events': len(all_events_info),
            'target_fields': self.target_fields,
            'events': all_events_info
        }
        
        with open("tokyo_events_ten_fields.json", 'w', encoding='utf-8') as f:
            json.dump(final_result, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ‰ çˆ¬å–å®Œæˆï¼")
        print(f"ğŸ“Š æˆåŠŸçˆ¬å– {len(all_events_info)} ä¸ªæ´»åŠ¨çš„åé¡¹ä¿¡æ¯")
        print(f"ğŸ“ ç»“æœæ–‡ä»¶: tokyo_events_ten_fields.json")
        
        # æ‰“å°æ±‡æ€»è¡¨æ ¼
        print("\nğŸ“‹ çˆ¬å–ç»“æœæ±‡æ€»:")
        print("-" * 100)
        for i, event in enumerate(all_events_info, 1):
            print(f"{i:2d}. {event['åç§°'][:30]:<30} | {event['é–‹å‚¬æœŸé–“'][:20]:<20} | {event['é–‹å‚¬å ´æ‰€'][:30]:<30}")
        print("-" * 100)
        
        return all_events_info

async def main():
    crawler = SpecificEventsCrawler()
    await crawler.crawl_top_10_specific_events()

if __name__ == "__main__":
    asyncio.run(main()) 