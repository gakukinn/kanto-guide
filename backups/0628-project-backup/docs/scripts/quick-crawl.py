#!/usr/bin/env python3
"""
å¿«é€Ÿç½‘é¡µçˆ¬å–å·¥å…·
é€‚ç”¨äºæ—¥æœ¬æ—…æ¸¸æŒ‡å—é¡¹ç›®
"""

import asyncio
import sys
from crawl4ai import AsyncWebCrawler

async def quick_crawl(url):
    """å¿«é€Ÿçˆ¬å–å•ä¸ªç½‘é¡µ"""
    print(f"ğŸš€ å¼€å§‹çˆ¬å–: {url}")
    
    async with AsyncWebCrawler(verbose=True) as crawler:
        try:
            result = await crawler.arun(
                url=url,
                wait_for=2,
                remove_overlay_elements=True,
                process_iframes=True
            )
            
            if result.success:
                print(f"âœ… çˆ¬å–æˆåŠŸï¼")
                print(f"ğŸ“„ æ ‡é¢˜: {result.metadata.get('title', 'æœªæ‰¾åˆ°')}")
                print(f"ğŸ“ å†…å®¹é•¿åº¦: {len(result.markdown)} å­—ç¬¦")
                print(f"ğŸ”— URL: {result.url}")
                
                # ä¿å­˜åˆ°æ–‡ä»¶
                filename = f"crawled_{url.split('/')[-1][:20]}.md"
                with open(filename, 'w', encoding='utf-8') as f:
                    f.write(f"# çˆ¬å–æ¥æº: {url}\n\n")
                    f.write(result.markdown)
                
                print(f"ğŸ’¾ å†…å®¹å·²ä¿å­˜åˆ°: {filename}")
                
                # æ˜¾ç¤ºå‰500å­—ç¬¦é¢„è§ˆ
                print("\nğŸ“‹ å†…å®¹é¢„è§ˆ:")
                print("-" * 50)
                print(result.markdown[:500] + "..." if len(result.markdown) > 500 else result.markdown)
                print("-" * 50)
                
                return result.markdown
            else:
                print(f"âŒ çˆ¬å–å¤±è´¥: {result.error_message}")
                return None
                
        except Exception as e:
            print(f"âŒ å‘ç”Ÿé”™è¯¯: {e}")
            return None

def main():
    """ä¸»å‡½æ•°"""
    if len(sys.argv) != 2:
        print("ä½¿ç”¨æ–¹æ³•: python quick-crawl.py <URL>")
        print("ç¤ºä¾‹: python quick-crawl.py https://www.jalan.net/")
        sys.exit(1)
    
    url = sys.argv[1]
    if not url.startswith(('http://', 'https://')):
        url = 'https://' + url
    
    result = asyncio.run(quick_crawl(url))
    
    if result:
        print("\nğŸ‰ çˆ¬å–å®Œæˆï¼æ‚¨å¯ä»¥åœ¨ Cursor ä¸­æŸ¥çœ‹ä¿å­˜çš„æ–‡ä»¶ã€‚")
    else:
        print("\nğŸ˜ çˆ¬å–å¤±è´¥ï¼Œè¯·æ£€æŸ¥URLæˆ–ç½‘ç»œè¿æ¥ã€‚")

if __name__ == "__main__":
    main() 