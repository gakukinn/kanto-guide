import asyncio
from crawl4ai import AsyncWebCrawler

async def test_crawl():
    print("ğŸš€ æµ‹è¯• Crawl4AI...")
    
    async with AsyncWebCrawler(verbose=True) as crawler:
        try:
            # æµ‹è¯•çˆ¬å–ä¸€ä¸ªç®€å•çš„ç½‘é¡µ
            result = await crawler.arun(
                url="https://httpbin.org/html",
                wait_for=2
            )
            
            if result.success:
                print("âœ… çˆ¬å–æˆåŠŸï¼")
                print(f"ğŸ“„ é¡µé¢æ ‡é¢˜: {result.metadata.get('title', 'æœªæ‰¾åˆ°æ ‡é¢˜')}")
                print(f"ğŸ“ å†…å®¹é•¿åº¦: {len(result.markdown)} å­—ç¬¦")
                print(f"ğŸ”— URL: {result.url}")
                print("\nğŸ“‹ éƒ¨åˆ†å†…å®¹:")
                print(result.markdown[:500] + "..." if len(result.markdown) > 500 else result.markdown)
                return True
            else:
                print("âŒ çˆ¬å–å¤±è´¥")
                print(f"é”™è¯¯: {result.error_message}")
                return False
                
        except Exception as e:
            print(f"âŒ å‘ç”Ÿé”™è¯¯: {e}")
            return False

if __name__ == "__main__":
    success = asyncio.run(test_crawl())
    if success:
        print("\nğŸ‰ Crawl4AI å®‰è£…æˆåŠŸå¹¶æ­£å¸¸å·¥ä½œï¼")
    else:
        print("\nğŸ˜ å®‰è£…å¯èƒ½æœ‰é—®é¢˜ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯") 