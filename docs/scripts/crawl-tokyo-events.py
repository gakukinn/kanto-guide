#!/usr/bin/env python3
"""
ä¸œäº¬æ´»åŠ¨ä¿¡æ¯çˆ¬å–å·¥å…·
ä¸“é—¨çˆ¬å– jalan.net ä¸œäº¬æ´»åŠ¨é¡µé¢çš„å‰10ä¸ªæ´»åŠ¨è¯¦æƒ…
"""

import asyncio
import json
import re
from datetime import datetime
from crawl4ai import AsyncWebCrawler
from crawl4ai.extraction_strategy import JsonCssExtractionStrategy

class TokyoEventsCrawler:
    def __init__(self):
        self.base_url = "https://www.jalan.net/event/130000/?screenId=OUW1025"
        self.events_data = []
    
    async def extract_event_links(self):
        """ä»ä¸»é¡µé¢æå–å‰10ä¸ªæ´»åŠ¨çš„é“¾æ¥"""
        print("ğŸ” æ­£åœ¨æå–æ´»åŠ¨é“¾æ¥...")
        
        # å®šä¹‰æå–æ´»åŠ¨é“¾æ¥çš„ç­–ç•¥
        schema = {
            "name": "event_links",
            "baseSelector": "body",
            "fields": [
                {
                    "name": "event_items",
                    "selector": ".event-item, .eventListItem, [class*='event'], [class*='item']",
                    "type": "nested",
                    "fields": [
                        {"name": "title", "selector": "h3, .title, .eventTitle, a", "type": "text"},
                        {"name": "link", "selector": "a", "type": "attribute", "attribute": "href"},
                        {"name": "date", "selector": ".date, .period, time", "type": "text"},
                        {"name": "location", "selector": ".location, .area, .place", "type": "text"}
                    ]
                }
            ]
        }
        
        extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)
        
        async with AsyncWebCrawler(verbose=True) as crawler:
            try:
                result = await crawler.arun(
                    url=self.base_url,
                    extraction_strategy=extraction_strategy,
                    wait_for=3,
                    remove_overlay_elements=True
                )
                
                if result.success:
                    print("âœ… æˆåŠŸè·å–é¡µé¢å†…å®¹")
                    
                    # ä¿å­˜åŸå§‹é¡µé¢å†…å®¹ä»¥ä¾›åˆ†æ
                    with open("tokyo_events_page.md", "w", encoding="utf-8") as f:
                        f.write(f"# ä¸œäº¬æ´»åŠ¨é¡µé¢å†…å®¹\n")
                        f.write(f"# çˆ¬å–æ—¶é—´: {datetime.now()}\n")
                        f.write(f"# æºURL: {self.base_url}\n\n")
                        f.write(result.markdown)
                    
                    # ä»markdownä¸­æå–æ´»åŠ¨é“¾æ¥
                    event_links = self.parse_event_links_from_markdown(result.markdown)
                    
                    if result.extracted_content:
                        try:
                            extracted_data = json.loads(result.extracted_content)
                            print(f"ğŸ“Š æå–åˆ°çš„ç»“æ„åŒ–æ•°æ®: {extracted_data}")
                        except json.JSONDecodeError:
                            print("âš ï¸ ç»“æ„åŒ–æ•°æ®è§£æå¤±è´¥ï¼Œä½¿ç”¨markdownè§£æ")
                    
                    return event_links[:10]  # è¿”å›å‰10ä¸ª
                else:
                    print(f"âŒ è·å–é¡µé¢å¤±è´¥: {result.error_message}")
                    return []
                    
            except Exception as e:
                print(f"âŒ å‘ç”Ÿé”™è¯¯: {e}")
                return []
    
    def parse_event_links_from_markdown(self, markdown_content):
        """ä»markdownå†…å®¹ä¸­è§£ææ´»åŠ¨é“¾æ¥"""
        print("ğŸ” ä»markdownä¸­è§£ææ´»åŠ¨é“¾æ¥...")
        
        event_links = []
        lines = markdown_content.split('\n')
        
        current_event = {}
        event_counter = 0
        
        for line in lines:
            line = line.strip()
            
            # æŸ¥æ‰¾é“¾æ¥
            link_match = re.search(r'\[([^\]]+)\]\((https://[^)]+)\)', line)
            if link_match:
                title = link_match.group(1)
                url = link_match.group(2)
                
                # è¿‡æ»¤æ‰æ˜æ˜¾ä¸æ˜¯æ´»åŠ¨é¡µé¢çš„é“¾æ¥
                if any(keyword in url for keyword in ['/event/', '/kanko/', '/activity/']):
                    event_links.append({
                        'title': title,
                        'url': url,
                        'line_content': line
                    })
                    event_counter += 1
                    print(f"ğŸ“Œ æ‰¾åˆ°æ´»åŠ¨ {event_counter}: {title}")
                    
                    if event_counter >= 10:
                        break
        
        # å¦‚æœæ²¡æ‰¾åˆ°è¶³å¤Ÿçš„é“¾æ¥ï¼Œå°è¯•å…¶ä»–æ¨¡å¼
        if len(event_links) < 3:
            print("ğŸ”„ ä½¿ç”¨å¤‡ç”¨è§£ææ¨¡å¼...")
            # æŸ¥æ‰¾æ‰€æœ‰åŒ…å«æ•°å­—å¼€å¤´çš„è¡Œï¼ˆå¯èƒ½æ˜¯æ´»åŠ¨åˆ—è¡¨ï¼‰
            for i, line in enumerate(lines):
                if re.match(r'^\d+', line.strip()) and 'http' in line:
                    # æå–è¿™è¡Œçš„é“¾æ¥
                    urls = re.findall(r'https://[^\s)]+', line)
                    for url in urls:
                        if len(event_links) < 10:
                            event_links.append({
                                'title': f"æ´»åŠ¨ {len(event_links) + 1}",
                                'url': url.rstrip(')'),
                                'line_content': line
                            })
        
        print(f"ğŸ“Š æ€»å…±æ‰¾åˆ° {len(event_links)} ä¸ªæ´»åŠ¨é“¾æ¥")
        return event_links
    
    async def crawl_event_details(self, event_link):
        """çˆ¬å–å•ä¸ªæ´»åŠ¨çš„è¯¦ç»†ä¿¡æ¯"""
        print(f"ğŸ•·ï¸ æ­£åœ¨çˆ¬å–æ´»åŠ¨: {event_link['title']}")
        
        # å®šä¹‰æ´»åŠ¨è¯¦æƒ…æå–ç­–ç•¥
        schema = {
            "name": "event_details",
            "baseSelector": "body",
            "fields": [
                {"name": "title", "selector": "h1, .title, .event-title, .page-title", "type": "text"},
                {"name": "date", "selector": ".date, .period, .event-date, time, .schedule", "type": "text"},
                {"name": "location", "selector": ".location, .venue, .place, .address", "type": "text"},
                {"name": "description", "selector": ".description, .content, .detail, .summary, p", "type": "text"},
                {"name": "access", "selector": ".access, .transportation, .traffic", "type": "text"},
                {"name": "contact", "selector": ".contact, .tel, .phone, .inquiry", "type": "text"},
                {"name": "price", "selector": ".price, .fee, .cost, .charge", "type": "text"},
                {"name": "website", "selector": "a[href*='http']", "type": "attribute", "attribute": "href"},
                {"name": "images", "selector": "img", "type": "attribute", "attribute": "src"}
            ]
        }
        
        extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)
        
        async with AsyncWebCrawler(verbose=True) as crawler:
            try:
                result = await crawler.arun(
                    url=event_link['url'],
                    extraction_strategy=extraction_strategy,
                    wait_for=3,
                    remove_overlay_elements=True
                )
                
                if result.success:
                    event_data = {
                        'source_title': event_link['title'],
                        'source_url': event_link['url'],
                        'crawl_time': datetime.now().isoformat(),
                        'markdown_content': result.markdown,
                        'extracted_data': None
                    }
                    
                    if result.extracted_content:
                        try:
                            event_data['extracted_data'] = json.loads(result.extracted_content)
                        except json.JSONDecodeError:
                            print(f"âš ï¸ {event_link['title']} çš„ç»“æ„åŒ–æ•°æ®è§£æå¤±è´¥")
                    
                    return event_data
                else:
                    print(f"âŒ çˆ¬å– {event_link['title']} å¤±è´¥: {result.error_message}")
                    return None
                    
            except Exception as e:
                print(f"âŒ çˆ¬å– {event_link['title']} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
                return None
    
    async def crawl_top_10_events(self):
        """çˆ¬å–å‰10ä¸ªæ´»åŠ¨çš„å®Œæ•´ä¿¡æ¯"""
        print("ğŸš€ å¼€å§‹çˆ¬å–ä¸œäº¬å‰10ä¸ªæ´»åŠ¨...")
        
        # ç¬¬ä¸€æ­¥ï¼šè·å–æ´»åŠ¨é“¾æ¥
        event_links = await self.extract_event_links()
        
        if not event_links:
            print("âŒ æœªèƒ½è·å–åˆ°æ´»åŠ¨é“¾æ¥")
            return
        
        print(f"ğŸ“‹ å‡†å¤‡çˆ¬å– {len(event_links)} ä¸ªæ´»åŠ¨çš„è¯¦ç»†ä¿¡æ¯")
        
        # ç¬¬äºŒæ­¥ï¼šçˆ¬å–æ¯ä¸ªæ´»åŠ¨çš„è¯¦æƒ…
        all_events_data = []
        
        for i, event_link in enumerate(event_links, 1):
            print(f"\nğŸ“„ å¤„ç†ç¬¬ {i}/{len(event_links)} ä¸ªæ´»åŠ¨...")
            
            event_data = await self.crawl_event_details(event_link)
            if event_data:
                all_events_data.append(event_data)
                
                # ä¿å­˜å•ä¸ªæ´»åŠ¨çš„è¯¦ç»†ä¿¡æ¯
                filename = f"event_{i:02d}_{event_link['title'][:20].replace('/', '_')}.json"
                with open(filename, 'w', encoding='utf-8') as f:
                    json.dump(event_data, f, ensure_ascii=False, indent=2)
                print(f"ğŸ’¾ å·²ä¿å­˜: {filename}")
            
            # æ·»åŠ å»¶è¿Ÿé¿å…è¢«å°
            if i < len(event_links):
                print("â±ï¸ ç­‰å¾…3ç§’...")
                await asyncio.sleep(3)
        
        # ç¬¬ä¸‰æ­¥ï¼šä¿å­˜æ±‡æ€»ä¿¡æ¯
        summary_data = {
            'crawl_time': datetime.now().isoformat(),
            'source_url': self.base_url,
            'total_events': len(all_events_data),
            'events': all_events_data
        }
        
        with open("tokyo_events_summary.json", 'w', encoding='utf-8') as f:
            json.dump(summary_data, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ‰ çˆ¬å–å®Œæˆï¼")
        print(f"ğŸ“Š æˆåŠŸçˆ¬å– {len(all_events_data)} ä¸ªæ´»åŠ¨")
        print(f"ğŸ“ æ±‡æ€»æ–‡ä»¶: tokyo_events_summary.json")
        
        return all_events_data

async def main():
    crawler = TokyoEventsCrawler()
    await crawler.crawl_top_10_events()

if __name__ == "__main__":
    asyncio.run(main()) 