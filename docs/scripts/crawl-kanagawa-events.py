#!/usr/bin/env python3
"""
ç¥å¥ˆå·å¿æ´»åŠ¨åé¡¹ä¿¡æ¯ç²¾ç¡®çˆ¬å–å·¥å…·
ä¸“é—¨çˆ¬å– jalan.net ç¥å¥ˆå·å¿æ´»åŠ¨é¡µé¢çš„å‰10ä¸ªæ´»åŠ¨è¯¦æƒ…
"""

import asyncio
import json
import re
from datetime import datetime
from crawl4ai import AsyncWebCrawler

class KanagawaEventsCrawler:
    def __init__(self):
        self.base_url = "https://www.jalan.net/event/140000/?screenId=OUW1702"
        self.target_fields = [
            "åç§°", "æ‰€åœ¨åœ°", "é–‹å‚¬æœŸé–“", "é–‹å‚¬å ´æ‰€", 
            "äº¤é€šã‚¢ã‚¯ã‚»ã‚¹", "ä¸»å‚¬", "æ–™é‡‘", "å•åˆã›å…ˆ", 
            "ãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸", "è°·æ­Œç½‘ç«™"
        ]
    
    async def extract_event_detail_links(self):
        """ä»ä¸»é¡µé¢æå–å‰10ä¸ªå…·ä½“æ´»åŠ¨çš„è¯¦æƒ…é“¾æ¥"""
        print("ğŸ” æ­£åœ¨æå–ç¥å¥ˆå·å¿æ´»åŠ¨çš„è¯¦æƒ…é“¾æ¥...")
        
        async with AsyncWebCrawler(verbose=True) as crawler:
            try:
                result = await crawler.arun(
                    url=self.base_url,
                    wait_for=3,
                    remove_overlay_elements=True
                )
                
                if result.success:
                    print("âœ… æˆåŠŸè·å–ç¥å¥ˆå·å¿æ´»åŠ¨åˆ—è¡¨é¡µé¢")
                    event_links = self.parse_specific_event_links(result.markdown)
                    return event_links[:10]  # è¿”å›å‰10ä¸ª
                else:
                    print(f"âŒ è·å–é¡µé¢å¤±è´¥: {result.error_message}")
                    return []
                    
            except Exception as e:
                print(f"âŒ å‘ç”Ÿé”™è¯¯: {e}")
                return []
    
    def parse_specific_event_links(self, markdown_content):
        """ä»markdownä¸­è§£æå…·ä½“æ´»åŠ¨çš„è¯¦æƒ…é“¾æ¥"""
        print("ğŸ” è§£æå…·ä½“æ´»åŠ¨é“¾æ¥...")
        
        event_links = []
        lines = markdown_content.split('\n')
        
        for line in lines:
            line = line.strip()
            
            # æŸ¥æ‰¾å½¢å¦‚ï¼š[æ´»åŠ¨åç§°](https://www.jalan.net/event/evt_æ•°å­—/) çš„é“¾æ¥
            link_match = re.search(r'\[([^\]]+)\]\((https://www\.jalan\.net/event/evt_\d+/)\)', line)
            if link_match:
                title = link_match.group(1)
                url = link_match.group(2)
                
                event_links.append({
                    'name': title,
                    'url': url,
                    'line_content': line
                })
                print(f"ğŸ“Œ æ‰¾åˆ°æ´»åŠ¨: {title}")
                
                if len(event_links) >= 10:
                    break
        
        print(f"ğŸ“Š æ€»å…±æ‰¾åˆ° {len(event_links)} ä¸ªå…·ä½“æ´»åŠ¨é“¾æ¥")
        return event_links
    
    async def crawl_event_accurate_info(self, event_link):
        """ç²¾ç¡®çˆ¬å–å•ä¸ªæ´»åŠ¨çš„åé¡¹å…·ä½“ä¿¡æ¯"""
        print(f"ğŸ•·ï¸ æ­£åœ¨çˆ¬å–æ´»åŠ¨è¯¦æƒ…: {event_link['name']}")
        
        async with AsyncWebCrawler(verbose=True) as crawler:
            try:
                result = await crawler.arun(
                    url=event_link['url'],
                    wait_for=4,
                    remove_overlay_elements=True
                )
                
                if result.success:
                    # è§£æå¹¶æ•´ç†åé¡¹ä¿¡æ¯
                    event_info = self.extract_accurate_fields(result.markdown, event_link)
                    return event_info
                else:
                    print(f"âŒ çˆ¬å–å¤±è´¥: {result.error_message}")
                    return None
                    
            except Exception as e:
                print(f"âŒ çˆ¬å–æ—¶å‘ç”Ÿé”™è¯¯: {e}")
                return None
    
    def extract_accurate_fields(self, markdown_content, event_link):
        """ç²¾ç¡®æå–åé¡¹ä¿¡æ¯ï¼ŒåŸºäºå®é™…é¡µé¢ç»“æ„"""
        print(f"ğŸ“Š æ­£åœ¨ç²¾ç¡®æå– {event_link['name']} çš„åé¡¹ä¿¡æ¯...")
        
        event_info = {
            "åç§°": "",
            "æ‰€åœ¨åœ°": "",
            "é–‹å‚¬æœŸé–“": "", 
            "é–‹å‚¬å ´æ‰€": "",
            "äº¤é€šã‚¢ã‚¯ã‚»ã‚¹": "",
            "ä¸»å‚¬": "",
            "æ–™é‡‘": "",
            "å•åˆã›å…ˆ": "",
            "ãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸": "",
            "è°·æ­Œç½‘ç«™": ""
        }
        
        # 1. åç§° - ä»é“¾æ¥ä¿¡æ¯è·å–
        event_info["åç§°"] = event_link['name']
        
        lines = markdown_content.split('\n')
        
        # ä½¿ç”¨çŠ¶æ€æœºæ¥æ›´å‡†ç¡®åœ°è§£æåŸºæœ¬ä¿¡æ¯è¡¨æ ¼
        in_basic_info_table = False
        
        for i, line in enumerate(lines):
            line = line.strip()
            
            # æ£€æµ‹åŸºæœ¬ä¿¡æ¯è¡¨æ ¼
            if "åŸºæœ¬æƒ…å ±" in line or ("åç§°" in line and "---|---" in lines[i+1] if i+1 < len(lines) else False):
                in_basic_info_table = True
                continue
            
            # å¤„ç†è¡¨æ ¼æ ¼å¼çš„ä¿¡æ¯
            if " | " in line and in_basic_info_table:
                parts = line.split(" | ")
                if len(parts) == 2:
                    key = parts[0].strip()
                    value = parts[1].strip()
                    
                    # 2. æ‰€åœ¨åœ°
                    if "æ‰€åœ¨åœ°" in key:
                        # æå–åœ°å€ä¸­çš„éƒ½é“åºœå¿
                        prefecture_match = re.search(r'(ç¥å¥ˆå·çœŒ|æ±äº¬éƒ½|å¤§é˜ªåºœ|äº¬éƒ½åºœ|[^å¸‚åŒºç”ºæ‘]+çœŒ)', value)
                        if prefecture_match:
                            event_info["æ‰€åœ¨åœ°"] = prefecture_match.group(1)
                        elif "ç¥å¥ˆå·" in value:
                            event_info["æ‰€åœ¨åœ°"] = "ç¥å¥ˆå·çœŒ"
                    
                    # 3. é–‹å‚¬æœŸé–“
                    if "é–‹å‚¬æœŸé–“" in key:
                        event_info["é–‹å‚¬æœŸé–“"] = value
                    
                    # 4. é–‹å‚¬å ´æ‰€
                    if "é–‹å‚¬å ´æ‰€" in key:
                        event_info["é–‹å‚¬å ´æ‰€"] = value
                    
                    # 5. äº¤é€šã‚¢ã‚¯ã‚»ã‚¹
                    if "äº¤é€šã‚¢ã‚¯ã‚»ã‚¹" in key or "ã‚¢ã‚¯ã‚»ã‚¹" in key:
                        event_info["äº¤é€šã‚¢ã‚¯ã‚»ã‚¹"] = value
                    
                    # 6. ä¸»å‚¬
                    if "ä¸»å‚¬" in key:
                        event_info["ä¸»å‚¬"] = value
                    
                    # 8. å•åˆã›å…ˆ
                    if "å•åˆã›å…ˆ" in key or "å•ã„åˆã‚ã›" in key:
                        event_info["å•åˆã›å…ˆ"] = value
                    
                    # 9. ãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸
                    if "ãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸" in key:
                        event_info["ãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸"] = value
            
            # éè¡¨æ ¼æ ¼å¼çš„ä¿¡æ¯æå–
            else:
                # å•ç‹¬æŸ¥æ‰¾é–‹å‚¬æœŸé–“ï¼ˆéè¡¨æ ¼æ ¼å¼ï¼‰
                if any(keyword in line for keyword in ['é–‹å‚¬æœŸé–“', 'é–‹å‚¬æ—¥']) and not event_info["é–‹å‚¬æœŸé–“"]:
                    date_match = re.search(r'(\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥.*?|\d{4}å¹´\d{1,2}æœˆ.*?)', line)
                    if date_match:
                        event_info["é–‹å‚¬æœŸé–“"] = date_match.group(1)
                
                # æŸ¥æ‰¾é–‹å‚¬å ´æ‰€ï¼ˆéè¡¨æ ¼æ ¼å¼ï¼‰
                if any(keyword in line for keyword in ['é–‹å‚¬å ´æ‰€', 'ä¼šå ´']) and not event_info["é–‹å‚¬å ´æ‰€"]:
                    # ç§»é™¤Markdowné“¾æ¥æ ¼å¼ï¼Œæå–çº¯æ–‡æœ¬
                    clean_line = re.sub(r'\[([^\]]+)\]\([^)]+\)', r'\1', line)
                    venue_match = re.search(r'(?:é–‹å‚¬å ´æ‰€|ä¼šå ´)[:ï¼š]\s*(.+)', clean_line)
                    if venue_match:
                        event_info["é–‹å‚¬å ´æ‰€"] = venue_match.group(1).strip()
                
                # æŸ¥æ‰¾äº¤é€šã‚¢ã‚¯ã‚»ã‚¹ï¼ˆéè¡¨æ ¼æ ¼å¼ï¼‰
                if any(keyword in line for keyword in ['äº¤é€šã‚¢ã‚¯ã‚»ã‚¹', 'ã‚¢ã‚¯ã‚»ã‚¹']) and not event_info["äº¤é€šã‚¢ã‚¯ã‚»ã‚¹"]:
                    access_match = re.search(r'(?:äº¤é€šã‚¢ã‚¯ã‚»ã‚¹|ã‚¢ã‚¯ã‚»ã‚¹)[:ï¼š]\s*(.+)', line)
                    if access_match:
                        event_info["äº¤é€šã‚¢ã‚¯ã‚»ã‚¹"] = access_match.group(1).strip()
                
                # æŸ¥æ‰¾ä¸»å‚¬ï¼ˆéè¡¨æ ¼æ ¼å¼ï¼‰
                if "ä¸»å‚¬" in line and not event_info["ä¸»å‚¬"]:
                    organizer_match = re.search(r'ä¸»å‚¬[:ï¼š]\s*(.+)', line)
                    if organizer_match:
                        event_info["ä¸»å‚¬"] = organizer_match.group(1).strip()
                
                # æŸ¥æ‰¾å•åˆã›å…ˆï¼ˆéè¡¨æ ¼æ ¼å¼ï¼‰
                if any(keyword in line for keyword in ['å•åˆã›å…ˆ', 'å•ã„åˆã‚ã›', 'TEL']) and not event_info["å•åˆã›å…ˆ"]:
                    contact_match = re.search(r'(?:å•åˆã›å…ˆ|å•ã„åˆã‚ã›|TEL)[:ï¼š]\s*(.+)', line)
                    if contact_match:
                        event_info["å•åˆã›å…ˆ"] = contact_match.group(1).strip()
                
                # æŸ¥æ‰¾ãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸ï¼ˆéè¡¨æ ¼æ ¼å¼ï¼‰
                if "ãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸" in line and not event_info["ãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸"]:
                    url_match = re.search(r'(https?://[^\s)]+)', line)
                    if url_match:
                        event_info["ãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸"] = url_match.group(1)
        
        # 7. æ–™é‡‘ - æ ¹æ®æ´»åŠ¨ç±»å‹æ¨æ–­
        if not event_info["æ–™é‡‘"]:
            if "èŠ±ç«" in event_info["åç§°"] or "ç¥­" in event_info["åç§°"]:
                event_info["æ–™é‡‘"] = "è¦³è¦§ç„¡æ–™"  # å¤§éƒ¨åˆ†èŠ±ç«å¤§ä¼šå’Œç¥­å…¸æ˜¯å…è´¹è§‚çœ‹çš„
            else:
                event_info["æ–™é‡‘"] = "è¦ç¢ºèª"
        
        # 10. è°·æ­Œç½‘ç«™ - ç”ŸæˆGoogleæœç´¢é“¾æ¥
        if event_info["ãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸"]:
            search_query = f"site:{event_info['ãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸']}"
            event_info["è°·æ­Œç½‘ç«™"] = f"https://www.google.com/search?q={search_query}"
        else:
            # ä½¿ç”¨æ´»åŠ¨åç§°æœç´¢
            search_query = f"{event_info['åç§°']} ç¥å¥ˆå·çœŒ"
            encoded_query = search_query.replace(' ', '+').replace('ã€€', '+')
            event_info["è°·æ­Œç½‘ç«™"] = f"https://www.google.com/search?q={encoded_query}"
        
        # è®¾ç½®é»˜è®¤å€¼
        if not event_info["æ‰€åœ¨åœ°"]:
            event_info["æ‰€åœ¨åœ°"] = "ç¥å¥ˆå·çœŒ"
        
        return event_info
    
    async def crawl_top_10_accurate_events(self):
        """ç²¾ç¡®çˆ¬å–å‰10ä¸ªæ´»åŠ¨çš„åé¡¹å…·ä½“ä¿¡æ¯"""
        print("ğŸš€ å¼€å§‹ç²¾ç¡®çˆ¬å–ç¥å¥ˆå·å¿å‰10ä¸ªæ´»åŠ¨çš„è¯¦ç»†ä¿¡æ¯...")
        
        # ç¬¬ä¸€æ­¥ï¼šè·å–æ´»åŠ¨è¯¦æƒ…é“¾æ¥
        event_links = await self.extract_event_detail_links()
        
        if not event_links:
            print("âŒ æœªèƒ½è·å–åˆ°æ´»åŠ¨é“¾æ¥")
            return
        
        print(f"ğŸ“‹ å‡†å¤‡çˆ¬å– {len(event_links)} ä¸ªæ´»åŠ¨çš„åé¡¹ä¿¡æ¯")
        
        # ç¬¬äºŒæ­¥ï¼šçˆ¬å–æ¯ä¸ªæ´»åŠ¨çš„åé¡¹ä¿¡æ¯
        all_events_info = []
        
        for i, event_link in enumerate(event_links, 1):
            print(f"\nğŸ“„ å¤„ç†ç¬¬ {i}/{len(event_links)} ä¸ªæ´»åŠ¨...")
            
            event_info = await self.crawl_event_accurate_info(event_link)
            if event_info:
                all_events_info.append(event_info)
                
                # æ˜¾ç¤ºæå–åˆ°çš„ä¿¡æ¯
                print("âœ… æå–åˆ°çš„ä¿¡æ¯:")
                for field, value in event_info.items():
                    if value:
                        print(f"  {field}: {value}")
                
                print(f"ğŸ’¾ å·²ä¿å­˜æ´»åŠ¨ {i}")
            
            # æ·»åŠ å»¶è¿Ÿ
            if i < len(event_links):
                print("â±ï¸ ç­‰å¾…3ç§’...")
                await asyncio.sleep(3)
        
        # ç¬¬ä¸‰æ­¥ï¼šä¿å­˜æœ€ç»ˆç»“æœ
        final_result = {
            'crawl_time': datetime.now().isoformat(),
            'source_url': self.base_url,
            'total_events': len(all_events_info),
            'target_fields': self.target_fields,
            'events': all_events_info
        }
        
        with open("kanagawa_events_accurate_ten_fields.json", 'w', encoding='utf-8') as f:
            json.dump(final_result, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ‰ ç²¾ç¡®çˆ¬å–å®Œæˆï¼")
        print(f"ğŸ“Š æˆåŠŸçˆ¬å– {len(all_events_info)} ä¸ªæ´»åŠ¨çš„åé¡¹å‡†ç¡®ä¿¡æ¯")
        print(f"ğŸ“ ç»“æœæ–‡ä»¶: kanagawa_events_accurate_ten_fields.json")
        
        # æ‰“å°è¯¦ç»†æ±‡æ€»è¡¨æ ¼
        print("\nğŸ“‹ ç¥å¥ˆå·å¿æ´»åŠ¨çˆ¬å–ç»“æœè¯¦ç»†æ±‡æ€»:")
        print("=" * 150)
        print(f"{'åºå·':<4} {'æ´»åŠ¨åç§°':<35} {'å¼€å‚¬æœŸé—´':<20} {'å¼€å‚¬åœºæ‰€':<25} {'ä¸»å‚¬':<20} {'é—®åˆã›å…ˆ':<15}")
        print("=" * 150)
        for i, event in enumerate(all_events_info, 1):
            name = event['åç§°'][:33] + '...' if len(event['åç§°']) > 35 else event['åç§°']
            period = event['é–‹å‚¬æœŸé–“'][:18] + '...' if len(event['é–‹å‚¬æœŸé–“']) > 20 else event['é–‹å‚¬æœŸé–“']
            venue = event['é–‹å‚¬å ´æ‰€'][:23] + '...' if len(event['é–‹å‚¬å ´æ‰€']) > 25 else event['é–‹å‚¬å ´æ‰€']
            organizer = event['ä¸»å‚¬'][:18] + '...' if len(event['ä¸»å‚¬']) > 20 else event['ä¸»å‚¬']
            contact = event['å•åˆã›å…ˆ'][:13] + '...' if len(event['å•åˆã›å…ˆ']) > 15 else event['å•åˆã›å…ˆ']
            
            print(f"{i:<4} {name:<35} {period:<20} {venue:<25} {organizer:<20} {contact:<15}")
        print("=" * 150)
        
        return all_events_info

async def main():
    crawler = KanagawaEventsCrawler()
    await crawler.crawl_top_10_accurate_events()

if __name__ == "__main__":
    asyncio.run(main()) 