import asyncio
import json
import os
from datetime import datetime
from crawl4ai import AsyncWebCrawler
from crawl4ai.extraction_strategy import JsonCssExtractionStrategy

class JapanGuideWebCrawler:
    """ä¸“é—¨ä¸ºæ—¥æœ¬æ—…æ¸¸æŒ‡å—é¡¹ç›®è®¾è®¡çš„ç½‘é¡µçˆ¬è™«"""
    
    def __init__(self):
        self.output_dir = "crawled-data"
        os.makedirs(self.output_dir, exist_ok=True)
    
    async def crawl_event_page(self, url, event_type="hanabi"):
        """
        çˆ¬å–æ´»åŠ¨é¡µé¢ä¿¡æ¯
        Args:
            url: ç›®æ ‡ç½‘é¡µURL
            event_type: æ´»åŠ¨ç±»å‹ (hanabi, matsuri, hanami, etc.)
        """
        print(f"ğŸ•·ï¸ å¼€å§‹çˆ¬å– {event_type} é¡µé¢: {url}")
        
        # å®šä¹‰æå–ç­–ç•¥
        schema = {
            "name": "event_info",
            "baseSelector": "body",
            "fields": [
                {"name": "title", "selector": "h1, .title, .event-title", "type": "text"},
                {"name": "date", "selector": ".date, .event-date, time", "type": "text"},
                {"name": "location", "selector": ".location, .venue, .place", "type": "text"},
                {"name": "description", "selector": ".description, .content, .detail", "type": "text"},
                {"name": "access", "selector": ".access, .transportation", "type": "text"},
                {"name": "contact", "selector": ".contact, .tel, .phone", "type": "text"},
                {"name": "website", "selector": "a[href*='http']", "type": "attribute", "attribute": "href"},
                {"name": "images", "selector": "img", "type": "attribute", "attribute": "src"}
            ]
        }
        
        extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)
        
        async with AsyncWebCrawler(verbose=True) as crawler:
            try:
                result = await crawler.arun(
                    url=url,
                    extraction_strategy=extraction_strategy,
                    wait_for=3,
                    remove_overlay_elements=True
                )
                
                if result.success:
                    # ä¿å­˜åŸå§‹markdown
                    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                    filename = f"{event_type}_{timestamp}"
                    
                    # ä¿å­˜markdownå†…å®¹
                    with open(f"{self.output_dir}/{filename}.md", "w", encoding="utf-8") as f:
                        f.write(f"# çˆ¬å–æ—¶é—´: {datetime.now()}\n")
                        f.write(f"# æºURL: {url}\n\n")
                        f.write(result.markdown)
                    
                    # ä¿å­˜ç»“æ„åŒ–æ•°æ®
                    if result.extracted_content:
                        extracted_data = json.loads(result.extracted_content)
                        with open(f"{self.output_dir}/{filename}_structured.json", "w", encoding="utf-8") as f:
                            json.dump({
                                "url": url,
                                "crawl_time": datetime.now().isoformat(),
                                "event_type": event_type,
                                "data": extracted_data
                            }, f, ensure_ascii=False, indent=2)
                    
                    print(f"âœ… çˆ¬å–æˆåŠŸï¼æ–‡ä»¶ä¿å­˜ä¸º: {filename}")
                    return {
                        "success": True,
                        "markdown": result.markdown,
                        "structured_data": result.extracted_content,
                        "filename": filename
                    }
                else:
                    print(f"âŒ çˆ¬å–å¤±è´¥: {result.error_message}")
                    return {"success": False, "error": result.error_message}
                    
            except Exception as e:
                print(f"âŒ å‘ç”Ÿé”™è¯¯: {e}")
                return {"success": False, "error": str(e)}
    
    async def crawl_multiple_pages(self, urls, event_type="hanabi"):
        """æ‰¹é‡çˆ¬å–å¤šä¸ªé¡µé¢"""
        results = []
        for i, url in enumerate(urls, 1):
            print(f"\nğŸ“„ å¤„ç†ç¬¬ {i}/{len(urls)} ä¸ªé¡µé¢...")
            result = await self.crawl_event_page(url, event_type)
            results.append(result)
            
            # æ·»åŠ å»¶è¿Ÿé¿å…è¢«å°
            if i < len(urls):
                print("â±ï¸ ç­‰å¾…3ç§’...")
                await asyncio.sleep(3)
        
        return results
    
    async def search_and_crawl(self, base_url, search_terms, event_type="hanabi"):
        """æœç´¢å¹¶çˆ¬å–ç›¸å…³é¡µé¢"""
        print(f"ğŸ” åœ¨ {base_url} æœç´¢: {search_terms}")
        
        async with AsyncWebCrawler(verbose=True) as crawler:
            # é¦–å…ˆçˆ¬å–ä¸»é¡µé¢å¯»æ‰¾ç›¸å…³é“¾æ¥
            result = await crawler.arun(url=base_url, wait_for=2)
            
            if result.success:
                # è¿™é‡Œå¯ä»¥æ·»åŠ é“¾æ¥æå–é€»è¾‘
                print("ğŸ”— æ‰¾åˆ°ç›¸å…³é“¾æ¥...")
                # å®é™…é¡¹ç›®ä¸­å¯ä»¥è§£æHTMLå¯»æ‰¾åŒ…å«æœç´¢è¯çš„é“¾æ¥
                return result.markdown
            else:
                print(f"âŒ æœç´¢å¤±è´¥: {result.error_message}")
                return None

# ä½¿ç”¨ç¤ºä¾‹
async def main():
    crawler = JapanGuideWebCrawler()
    
    # ç¤ºä¾‹ï¼šçˆ¬å–èŠ±ç«å¤§ä¼šä¿¡æ¯
    test_urls = [
        "https://www.jalan.net/news/article/522677/",  # ç¤ºä¾‹URL
        # å¯ä»¥æ·»åŠ æ›´å¤šURL
    ]
    
    print("ğŸš€ å¼€å§‹æ‰¹é‡çˆ¬å–...")
    results = await crawler.crawl_multiple_pages(test_urls, "hanabi")
    
    success_count = sum(1 for r in results if r["success"])
    print(f"\nğŸ“Š çˆ¬å–å®Œæˆï¼æˆåŠŸ: {success_count}/{len(results)}")

if __name__ == "__main__":
    asyncio.run(main()) 